{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from data_block import *\n",
    "from transforms import *\n",
    "from transformers import GPT2Tokenizer\n",
    "from model import *\n",
    "from learner import *\n",
    "from callbacks import *\n",
    "# from xresnets import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [into_rgb, ResizeFixed(224), np_to_float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('C:/Users/iamab/Downloads/datasets/coco/')\n",
    "il = ImageList.from_path(path, tfms = tfms, include = ['train2014', 'val2014'])\n",
    "sl = SplitDataset.from_ratio(il, ratio=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitDataset\n",
       "Train: ImageList (117123 items)\n",
       "[WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000018873.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000151651.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000171382.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000478511.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000092017.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000299712.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000055074.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000524790.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000255708.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000170292.jpg')...]\n",
       "Valid: ImageList (6164 items)\n",
       "[WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000080168.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000540784.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000034816.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000198826.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/val2014/COCO_val2014_000000184446.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000339550.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000244418.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000053591.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000172226.jpg'), WindowsPath('C:/Users/iamab/Downloads/datasets/coco/train2014/COCO_train2014_000000472181.jpg')...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pic_and_caption(l, index):\n",
    "    tok = l.py.tokenizer\n",
    "    im = l.x[index]\n",
    "    label = l.y.items[index]\n",
    "    print(tok.decode(label.numpy(), skip_special_tokens = True))\n",
    "    plt.imshow(im.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "    \n",
    "# sh = partial(show_pic_and_caption, il)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        ls = [i.strip() for i in f.readlines()]\n",
    "        return ls\n",
    "    \n",
    "def conv_to_dict(file):\n",
    "    dic = {}\n",
    "    for i in file:\n",
    "        j = i.split('\\t')\n",
    "        dic[j[0]] = j[1]\n",
    "    return dic\n",
    "\n",
    "def caption_label(o, label_dict,caption_number=1):\n",
    "    return label_dict[o.name + f'#{caption_number}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0716 08:56:26.379752  6424 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\iamab/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0716 08:56:26.380744  6424 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\iamab/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "E0716 08:56:26.587119  6424 tokenization_utils.py:548] Using pad_token, but it is not set yet.\n",
      "I0716 08:56:26.589106  6424 tokenization_utils.py:1177] Adding <BOS> to the vocabulary\n",
      "I0716 08:56:26.590106  6424 tokenization_utils.py:1255] Assigning <BOS> to the bos_token key of the tokenizer\n",
      "I0716 08:56:26.591102  6424 tokenization_utils.py:1177] Adding <EOS> to the vocabulary\n",
      "I0716 08:56:26.592118  6424 tokenization_utils.py:1255] Assigning <EOS> to the eos_token key of the tokenizer\n",
      "I0716 08:56:26.593097  6424 tokenization_utils.py:1177] Adding <PAD> to the vocabulary\n",
      "I0716 08:56:26.594094  6424 tokenization_utils.py:1255] Assigning <PAD> to the pad_token key of the tokenizer\n"
     ]
    }
   ],
   "source": [
    "ll = label_train_valid(sl, \n",
    "                      func = partial(caption_label, label_dict = conv_to_dict(read_file(path/'ftd/token.txt'))),\n",
    "                  procy = Tokprocessor(GPT2Tokenizer.from_pretrained('gpt2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(samples, pad_idx,flip = False):\n",
    "    images = torch.stack([s[0] for s in samples])\n",
    "    captions = [s[1].contiguous() for s in samples]\n",
    "    if flip:\n",
    "        for s in captions:\n",
    "            idx = next(j for j,n in enumerate(s) if n == pad_idx)\n",
    "            cut = s[:idx].flip(0)\n",
    "            s[:idx] = cut\n",
    "    res = torch.stack([s[1] for s in samples])\n",
    "            \n",
    "    return (torch.stack([s[0] for s in samples]), res.long()), torch.cat((res[:,1:], torch.zeros(res.size(0),1).int()+pad_idx), dim=1).long()\n",
    "\n",
    "def cap_databunchify(sd, train_bs = None,valid_bs = None,**kwargs):\n",
    "    return DataBunch(*get_cap_dls(sd, train_bs, valid_bs, collate_fn = pad_collate))\n",
    "\n",
    "SplitDataset.cap_databunch = cap_databunchify\n",
    "\n",
    "data = ll.cap_databunch(train_bs=8, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = partial(show_pic_and_caption,ll.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "mod = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with pretrained visual backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_caption_model(data, visual_head= mod, N = 4)\n",
    "\n",
    "loss = FlatLoss(nn.CrossEntropyLoss())\n",
    "\n",
    "cbfs = [CudaCapCallback, partial(AvgStatsCallback, cap_accuracy, AvgStatsCap), ProgressCallback, \n",
    "        normalise_callback(data, normalise_func= cap_normalise), NewRecorderCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, loss, cb_funcs= cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_cap_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_cap_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.066910</td>\n",
       "      <td>0.682687</td>\n",
       "      <td>5.070878</td>\n",
       "      <td>0.706860</td>\n",
       "      <td>20:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.504230</td>\n",
       "      <td>0.725086</td>\n",
       "      <td>1.949681</td>\n",
       "      <td>0.787381</td>\n",
       "      <td>20:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.929187</td>\n",
       "      <td>0.786250</td>\n",
       "      <td>1.791563</td>\n",
       "      <td>0.802081</td>\n",
       "      <td>20:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model((x[0].cuda(), x[1].cuda()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "mod = models.resnet34(pretrained= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_caption_model(data, visual_head= mod, N = 4)\n",
    "\n",
    "loss = FlatLoss(nn.CrossEntropyLoss())\n",
    "\n",
    "cbfs = [CudaCapCallback, partial(AvgStatsCallback, cap_accuracy, AvgStatsCap), ProgressCallback, \n",
    "        normalise_callback(data, normalise_func= cap_normalise), NewRecorderCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, loss, cb_funcs= cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_cap_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_cap_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.520174</td>\n",
       "      <td>0.704436</td>\n",
       "      <td>4.396590</td>\n",
       "      <td>0.726308</td>\n",
       "      <td>20:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.222858</td>\n",
       "      <td>0.722765</td>\n",
       "      <td>2.855051</td>\n",
       "      <td>0.726308</td>\n",
       "      <td>20:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.888842</td>\n",
       "      <td>0.724005</td>\n",
       "      <td>2.769075</td>\n",
       "      <td>0.726308</td>\n",
       "      <td>20:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
