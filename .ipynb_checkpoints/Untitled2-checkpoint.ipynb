{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 16:48:24.845788 10600 file_utils.py:39] PyTorch version 1.4.0 available.\n",
      "I0813 16:48:27.830925 10600 file_utils.py:55] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from data_block import *\n",
    "from transforms import *\n",
    "from transformers import GPT2Tokenizer\n",
    "from model import *\n",
    "from learner import *\n",
    "from callbacks import *\n",
    "from optimizers import *\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption_data(path, ann_path, tfms,image_folders, train_caption_file, valid_caption_file):\n",
    "    il = ImageList.from_path(path, tfms = tfms, include = image_folders)\n",
    "    train_ann = json.load(open(os.path.join(path, ann_path, train_caption_file)))\n",
    "    valid_ann = json.load(open(os.path.join(path, ann_path, valid_caption_file)))\n",
    "    dic = dict([(i['image_id'], i['caption']) for i in train_ann['annotations']])\n",
    "    dic.update([(i['image_id'], i['caption']) for i in valid_ann['annotations']])\n",
    "    return il, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "il, cap_dict = get_caption_data(path = 'C:/Users/iamab/Downloads/coco',\n",
    "                               ann_path= 'annotations_trainval2014/annotations',\n",
    "                               tfms = [into_rgb, ResizeFixed(224), np_to_float],\n",
    "                               image_folders = ['train_images2014', 'valid_images2014'],\n",
    "                               train_caption_file = 'captions_train2014.json',\n",
    "                               valid_caption_file = 'captions_val2014.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitDataset\n",
       "Train: ImageList (82783 items)\n",
       "[WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000009.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000025.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000030.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000034.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000036.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000049.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000061.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000064.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000071.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/train_images2014/train2014/COCO_train2014_000000000072.jpg')...]\n",
       "Valid: ImageList (40504 items)\n",
       "[WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000042.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000073.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000074.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000133.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000136.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000139.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000143.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000164.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000192.jpg'), WindowsPath('C:/Users/iamab/Downloads/coco/valid_images2014/val2014/COCO_val2014_000000000196.jpg')...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parent_splitter(o, name): return True if o.parent.name == name else False\n",
    "\n",
    "sl = SplitDataset.from_func(il, partial(parent_splitter, name = 'val2014'))\n",
    "\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 16:49:31.533961 10600 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\iamab/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0813 16:49:31.534923 10600 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\iamab/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "E0813 16:49:31.757920 10600 tokenization_utils.py:548] Using pad_token, but it is not set yet.\n",
      "I0813 16:49:31.758880 10600 tokenization_utils.py:1177] Adding <BOS> to the vocabulary\n",
      "I0813 16:49:31.759880 10600 tokenization_utils.py:1255] Assigning <BOS> to the bos_token key of the tokenizer\n",
      "I0813 16:49:31.760875 10600 tokenization_utils.py:1177] Adding <EOS> to the vocabulary\n",
      "I0813 16:49:31.761873 10600 tokenization_utils.py:1255] Assigning <EOS> to the eos_token key of the tokenizer\n",
      "I0813 16:49:31.763868 10600 tokenization_utils.py:1177] Adding <PAD> to the vocabulary\n",
      "I0813 16:49:31.766863 10600 tokenization_utils.py:1255] Assigning <PAD> to the pad_token key of the tokenizer\n"
     ]
    }
   ],
   "source": [
    "def get_captions(o,cap_dict):\n",
    "    im_id = int(re.findall(r'0*(\\d+).jpg', o.name)[0])\n",
    "    return cap_dict[im_id]\n",
    "\n",
    "ll = label_train_valid(sl, func = partial(get_captions, cap_dict = cap_dict),\n",
    "                       procy = Tokprocessor(GPT2Tokenizer.from_pretrained('gpt2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pic_and_caption(l, index):\n",
    "    tok = l.py.tokenizer\n",
    "    im = l.x[index]\n",
    "    label = tok.decode(l.y.items[index], skip_special_tokens = True)\n",
    "    print(label)\n",
    "    plt.imshow(im.permute(1,2,0))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = partial(show_pic_and_caption,ll.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pad_collate(samples, pad_idx,flip = False):\n",
    "    images = torch.stack([s[0] for s in samples])\n",
    "    if flip:\n",
    "        captions = [s[1].contiguous() for s in samples]\n",
    "        for s in captions:\n",
    "            idx = next(j for j,n in enumerate(s) if n == pad_idx)\n",
    "            cut = s[:idx].flip(0)\n",
    "            s[:idx] = cut\n",
    "    res = torch.stack([s[1] for s in samples])\n",
    "            \n",
    "    return (images, res.long()), torch.cat((res[:,1:], torch.zeros(res.size(0),1).int()+pad_idx), dim=1).long()\n",
    "\n",
    "def cap_databunchify(sd, train_bs = None,valid_bs = None,**kwargs):\n",
    "    return DataBunch(*get_cap_dls(sd, train_bs, valid_bs, collate_fn = pad_collate))\n",
    "\n",
    "SplitDataset.cap_databunch = cap_databunchify\n",
    "\n",
    "data = ll.cap_databunch(train_bs=16, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nit(o): return next(iter(o))\n",
    "\n",
    "# x,_= nit(data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/a_bhimanyu/yo\" target=\"_blank\">https://app.wandb.ai/a_bhimanyu/yo</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/a_bhimanyu/yo/runs/3a713aar\" target=\"_blank\">https://app.wandb.ai/a_bhimanyu/yo/runs/3a713aar</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 16:50:54.006582 11344 run_manager.py:681] file/dir created: C:/Users/iamab/Google Drive\\wandb\\run-20200813_165050-3a713aar\\wandb-metadata.json\n",
      "I0813 16:50:55.356783 10600 run_manager.py:928] system metrics and metadata threads started\n",
      "I0813 16:50:55.357784 10600 run_manager.py:937] checking resume status, waiting at most 10 seconds\n",
      "I0813 16:50:56.095515 10600 run_manager.py:955] resuming run from id: UnVuOnYxOjNhNzEzYWFyOnlvOmFfYmhpbWFueXU=\n",
      "I0813 16:50:56.199205 10600 run_manager.py:967] upserting run before process can begin, waiting at most 10 seconds\n",
      "I0813 16:50:57.147225  4284 run_manager.py:1052] saving patches\n",
      "I0813 16:50:57.150185  4284 run_manager.py:1056] saving pip packages\n",
      "I0813 16:50:57.153180  4284 run_manager.py:1058] initializing streaming files api\n",
      "I0813 16:50:57.157169  4284 run_manager.py:1065] unblocking file change observer, beginning sync with W&B servers\n",
      "I0813 16:50:57.223988 10600 run_manager.py:1072] shutting down system stats and metadata service\n",
      "I0813 16:50:57.358974 10600 run_manager.py:1086] stopping streaming files and file change observer\n",
      "I0813 16:50:58.015541 10600 run_manager.py:681] file/dir created: C:/Users/iamab/Google Drive\\wandb\\run-20200813_165050-3a713aar\\wandb-events.jsonl\n",
      "I0813 16:50:58.098034 10600 run_manager.py:681] file/dir created: C:/Users/iamab/Google Drive\\wandb\\run-20200813_165050-3a713aar\\wandb-history.jsonl\n",
      "I0813 16:50:58.102024 10600 run_manager.py:681] file/dir created: C:/Users/iamab/Google Drive\\wandb\\run-20200813_165050-3a713aar\\wandb-summary.json\n",
      "I0813 16:50:58.107010 10600 run_manager.py:681] file/dir created: C:/Users/iamab/Google Drive\\wandb\\run-20200813_165050-3a713aar\\requirements.txt\n",
      "I0813 16:50:58.110005 10600 run_manager.py:681] file/dir created: C:/Users/iamab/Google Drive\\wandb\\run-20200813_165050-3a713aar\\code\\_session_history.ipynb\n",
      "I0813 16:50:58.113991 10600 run_manager.py:681] file/dir created: C:/Users/iamab/Google Drive\\wandb\\run-20200813_165050-3a713aar\\code\n"
     ]
    }
   ],
   "source": [
    "visual_model = models.resnet50(pretrained= True, progress= True)\n",
    "\n",
    "cap_model = get_caption_model(data,num_visual_features=2048,\n",
    "                              visual_head=visual_model, N = 4, activation = 'gelu')\n",
    "\n",
    "optimizer = partial(StatefulOptimizer, \n",
    "                    steppers = [momentum_step, weight_decay,lookahead_step],\n",
    "                   stats = [AverageGrad(), Step(), LookAheadStat()],\n",
    "                   lr = 0.02, wd = 1e-04)\n",
    "\n",
    "class GradZeroCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        for p in self.model.visual_backbone.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "cbfs = [CudaCapCallback, partial(AvgStatsCallback, cap_accuracy, AvgStatsCap), ProgressCallback, \n",
    "        normalise_callback(data, normalise_func= cap_normalise), NewRecorderCallback, WandbCallback, \n",
    "        GradZeroCallback]\n",
    "\n",
    "wandb.init(name = 'vb_frozen', project = 'yo',entity = 'a_bhimanyu', \n",
    "          dir = 'C:/Users/iamab/Google Drive');\n",
    "\n",
    "loss = FlatLoss(nn.CrossEntropyLoss(ignore_index = cap_model.pad_tok_id))\n",
    "\n",
    "\n",
    "\n",
    "learn = Learner(cap_model, data, loss, optimizer, cb_funcs= cbfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
